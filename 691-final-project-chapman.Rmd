---
title: "691 Final Project"
author: "Eddie Chapman"
date: "May 11, 2018"
output:
  pdf_document:
    fig_caption: yes
    template: template4.tex
  word_document: default
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
linkcolor: 3510
documentclass: article
bibliography: 691-final-bibliography.bib
---

```{r setup, include=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo = FALSE, include=FALSE}
library(tidyverse)
setwd('C:\\Users\\chapman4\\Desktop\\wikipedia research')

cn <- read_delim('cn2-length.txt', 
                 delim  = '\t', 
                 na = c('NA'),
                 quoted_na = F,
                 col_names = c('Language.CN', 'Length.CN', 'Central.Node'))

ln <- read_delim('ln2-length.txt', 
                 delim = '\t', 
                 na = c('NA'),
                 quoted_na = F,
                 col_names = c('Language.CN', 'Local.Network', 'Length.LN', 'Central.Node'))

scn <- read_delim('scn2-length.txt', 
                  delim = '\t', 
                  na = c('NA'),
                  quoted_na = F,
                  col_names = c('Language.SCN', 'Semantic.Content.Node', 'Length.SCN', 'Language.CN', 'Central.Node'))

gn <- read_delim('gn2-length.txt', 
                 delim  = '\t', 
                 na = c('NA'),
                 quoted_na = F,
                 col_names = c('Language.SCN', 'Global.Network', 'Length.GN', 'Semantic.Content.Node', 'Language.CN', 'Central.Node'))

groups <- read_delim('cn-groups.txt', 
                   delim     = '\t', 
                   na = c('NA'),
                   quoted_na = F,
                   col_names = c('Group', 'Central.Node'))
```

```{r echo = FALSE, include=FALSE, eval=FALSE}
edits.scn <- read_delim('scn2-edits.txt',
                    delim = '\t',
                    quoted_na = F,
                    na = c('NA'),
                    col_names = c('Language.SCN', 'Timestamp', 'Semantic.Content.Node', 'Central.Node', 'Language.CN'))

edits.cn <- read_delim('cn2-edits.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Language.CN', 'Timestamp', 'Central.Node'))

edits.ln <- read_delim('ln2-edits.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Language.CN', 'Timestamp', 'Local.Network', 'Central.Node'))

edits.gn <- read_delim('gn2-edits.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Language.SCN', 'Global.Network', 'Timestamp', 'Semantic.Content.Node', 'Central.Node', 'Language.CN'))
```
```{r echo = FALSE, include=FALSE, eval=FALSE}
views.scn <- read_delim('scn2-views.txt',
                    delim = '\t',
                    quoted_na = F,
                    na = c('NA'),
                    col_names = c('Semantic.Content.Node', 'Central.Node', 'Language.SCN', 'Timestamp', 'Views'))

views.cn <- read_delim('cn2-views.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Central.Node', 'Language.CN', 'Timestamp', 'Views'))

views.ln <- read_delim('ln2-views.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Local.Network', 'Central.Node', 'Language.CN', 'Timestamp', 'Views'))

views.gn <- read_delim('gn2-views.txt',
                       delim = '\t',
                       quoted_na = F,
                       na = c('NA'),
                       col_names = c('Global.Network', 'Language.SCN', 'Central.Node', 'Timestamp', 'Views'))
```

## Summary

### Introduction
 
My project analyzes the growth and influence of Wikipedia articles related to the Alt-right. I use static and time-series article metadata to measure a group of articles across multiple network contexts. 

I use the following tools: 
  
* MediaWiki API for data collection  
  
* Python 3.6 for API interaction 
  
* R 3.4 for data cleaning and analysis

### Background and Questions

The Alt-right is an recent political movement that groups white-supremists, neo-Nazis, and other far-right ideologies. It appeals to many young people through internet culture as opposed to traditional right-wing politics. The movement gained considerable legitimacy through the Presidential campaign of Donald Trump. Many of his advisors and associates openly support and encourage the Alt-right. 

I was inspired to pursue the project after inspecting Nazi-related Wikipedia articles. I wanted to know if it was possible to identify recent trends in Nazi-related edit history. After researching the subject, I selected an article by Mirko Kämpf et al. that could serve as a framework for my project [-@kampf2015detection]. 

In *The Detection of Emerging Trends Using Wikipedia Traffic Data and Context Networks*, Kämpf et al. observe the growth in the Wikipedia articles of cloud-computing companies. They measure article length, linking, popularity, and edit-activity  within a single language as and throughout all language editions of Wikipedia. I chose to follow their process because it tracked the growth of multiple pages over time.

My goals for the project: 

*  Research and demonstrate Wikipedia metadata analysis techniques
*  Learn about APIs and build a program to access the MediaWiki API
*  Organize and manipulate a large set of static and time-series data

My initial research questions:  
  
*  In what language-editions of Wikipedia are Alt-Right topics best represented?   
*  Are there noticable trend differences between different subtopics of Alt-right Wikipedia articles?
*  How did the local and global representation of Alt-right Wikipedia articles change before and after the 2017 US election?  

My project is an exploration and demonstration of techniques more than a research project. Therefore, these research are preliminary and the questions and results would be strengthened through additional research and testing. 

### Data Selection

#### Temporal Scope

I chose January 1st, 2015 as the earliest date for any time-series data I collected. I used Google Trends to find the most recent date that might precede the growth of the movement. 

#### Topic Articles

My initial set of articles is drawn from English Wikipedia's "Category:Alt-right". I selected 36 articles and grouped them by subtopic for later analysis. The subtopics represent related influences and aspects of the Alt-right. Some of the predate the movement, while others are recent inventions. 

1. **Hate groups**  
Articles about white-supremicist and neo-nazi organizations, individuals, and websites. These groups make no attempt to appeal to mainstream interests. They generally pre-date the Alt-right.  
[Anti-Communist Action](https://en.wikipedia.org/wiki/Anti-Communist_Action); [Antipodean Resistance](https://en.wikipedia.org/wiki/Antipodean_Resistance); [Atomwaffen Division](https://en.wikipedia.org/wiki/Atomwaffen_Division); [The Daily Stormer](https://en.wikipedia.org/wiki/The_Daily_Stormer); [Identity Evropa](https://en.wikipedia.org/wiki/Identity_Evropa); [The Right Stuff (blog)](https://en.wikipedia.org/wiki/The_Right_Stuff_(blog)); [Stormfront (website)](https://en.wikipedia.org/wiki/Stormfront_(website)).

2. **Far-right ideologies**  
Articles describing variations of far-right political ideologies. Not associated with a specific organization or individual.  
[Alt-right](https://en.wikipedia.org/wiki/Alt-right); [Alt-lite](https://en.wikipedia.org/wiki/Alt-lite); [New Right](https://en.wikipedia.org/wiki/New_Right); [Nouvelle Droite](https://en.wikipedia.org/wiki/Nouvelle_Droite); [Technolibertarianism](https://en.wikipedia.org/wiki/Technolibertarianism).

3. **Trolls**  
Internet personalities that identify with the Alt-right, and their associated online venues. They mix internet humor with hatred. They may appeal to young people who do not specifically identify with white-supremicist or fascist ideologies.  
[Baked Alaska (entertainer)](https://en.wikipedia.org/wiki/Baked_Alaska_(entertainer)); [Gab (social network)](https://en.wikipedia.org/wiki/Gab_(social_network)); [Joshua Ryne Goldberg](https://en.wikipedia.org/wiki/Joshua_Ryne_Goldberg); [Millennial Woes](https://en.wikipedia.org/wiki/Millennial_Woes); [Jack Posobiec](https://en.wikipedia.org/wiki/Jack_Posobiec); [Weev](https://en.wikipedia.org/wiki/Weev).

4. **Memes**  
Internet jokes and slang associated with the Alt-right.  
  [Cuckservative](https://en.wikipedia.org/wiki/Cuckservative); [Generation Snowflake](https://en.wikipedia.org/wiki/Generation_Snowflake); [It's OK to be white](https://en.wikipedia.org/wiki/It%27s_OK_to_be_white); [Moon Man (Internet meme)](https://en.wikipedia.org/wiki/Moon_Man_(Internet_meme)); [Pepe the Frog](https://en.wikipedia.org/wiki/Pepe_the_Frog); [Snowflake (slang)](https://en.wikipedia.org/wiki/Snowflake_(slang)); [Trash Doves](https://en.wikipedia.org/wiki/Trash_Doves); [Triple parentheses](https://en.wikipedia.org/wiki/Triple_parentheses).

5. **Conspiracy theorists**  
Websites, organizations, and individuals that spread misinformation and have reached wider audiences with the rise of the Alt-right. Not strictly identified as alt-right, but generally associated with fringe far-right policitical thought.  
[Michael A. Hoffman II](https://en.wikipedia.org/wiki/Michael_A._Hoffman_II); [InfoWars](https://en.wikipedia.org/wiki/InfoWars); [Mike Cernovich](https://en.wikipedia.org/wiki/Mike_Cernovich); [Alex Jones](https://en.wikipedia.org/wiki/Alex_Jones); [Pizzagate conspiracy theory](https://en.wikipedia.org/wiki/Pizzagate_conspiracy_theory).

6. **Trump**  
Politicians, organizations, and websites connected to the Trump administration that identify as Alt-right or are celebrated by the Alt-right.  
[Steve Bannon](https://en.wikipedia.org/wiki/Steve_Bannon); [Breitbart News](https://en.wikipedia.org/wiki/Breitbart_News); [DeploraBall](https://en.wikipedia.org/wiki/DeploraBall); [David Duke](https://en.wikipedia.org/wiki/David_Duke); [Lion Guard](https://en.wikipedia.org/wiki/Lion_Guard).

#### Context Articles

In Kämpf et al.'s framework, the 36  articles above are known as **Central Nodes**. These represent the Alt-right in a single language, English. Using the 36 CN, three additional groups of Wikipedia articles are selected.  

1. **Semantic Core Nodes (SCN)**  
All articles that can be reached through interlanguage links in the CN. Interlanguage links connect articles across Wikipedia language editions when editors determine that they closely represent the same topic or material. These are the global counterparts of individual CNs.

2. **Local network (LN)**  
All articles that can be reached through Wikipedia article links from the CN. This is the local neighborhood surrounding the CN, restricted to English Wikipedia. 
  
3. **Global Network (GN)**  
All articles that can be reached from the SCN through Wikipedia article links. This is the global neighborhood surrounding the SCN, across all Wikipedia language editions. 

#### Article metadata

The following metadata was collected for each article in groups CN, SCN, LN, and GN: 

*  Article length in characters, using the most recent revision.  
  
*  Daily page-view totals.  
  
*  Daily edit-event totals.

The metadata was compiled into summaries for each of the 36 CN:

*  The number of nodes in each group (SCN, LN, and GN) that are connected to the CN 

*  Article length totals, by group (SCN, LN, GN) for nodes connected to the CN  

*  Daily page-view and edit-event totals, by group (SCN, LN, GN) for nodes connected to the CN 


#### MediaWiki API

Metadata was accessible through MediaWiki API. I used Python to build a program that would retieve the necessary information. 

The program contains the functions:
  
* Read a TSV file of page titles and their language edition codes  
  
* Query the MediaWiki API for each title
  
* Parse and store the query results  
  
* Write query results to a TSV file  
  

The `query()` function is adapted adapted from an example in the MediaWiki API documentation.[@mediawiki:continuingqueries].  

I adjusted the parameters for each stage of data collection. The following example retrieves the time and date of edit events.

```{python, eval=FALSE, error=FALSE}
def query(page, wiki):
    request = {}
    request['titles'] = page
    request['action'] = 'query'
    request['format'] = 'json'
    request['formatversion'] = '2'
    request['prop'] = 'revisions'
    request['rvlimit'] = 'max'
    request['rvprop'] = 'timestamp'
    request['rvend'] = '2015-01-01T00:00:00Z'
    lastContinue = {}
    (...)
```
  
The API will only return a maximum of 500 results per request. A `continue` value is included in the results in order to access the remaining values. This can be added to the request to return the next 500 values. The function uses a `while` loop to continue calling the API until all results have been delivered.

```{python, eval=FALSE, error=FALSE}
def query(page, wiki):
    (...)
    while True:
        # Clone original request
        req = request.copy()
        # Modify with the values returned in the 'continue' section of the last result.
        req.update(lastContinue)
        # Call API
        result = requests.get('https://' + wiki + '.wikipedia.org/w/api.php',\
                              params=req).json()
        if 'error' in result:
            print(result['error'])
        if 'warnings' in result:
            print(result['warnings'])
        if 'query' in result:
            try:
                yield result['query']['pages'][0]['revisions']
            except Exception:
                pass
        if 'continue' not in result:
            break
        lastContinue = result['continue']
```
  

### Data Structures

The MediaWiki API offers multiple export options. I chose JSON. Here is a sample of the results from the previous section:  

```json 
{
  'continue': {
    'rvcontinue': '20171228002800|817377590', 
    'continue': '||'
  }, 
  'query': {
    'pages': [
      {
        'pageid': 49273972, 
        'ns': 0, 
        'title': 'Alt-right', 
        'revisions': [
          {
            'timestamp': '2018-05-10T16:52:58Z'
          }, 
          {
            'timestamp': '2018-05-10T16:46:26Z'
          }, 
          {
            'timestamp': '2018-05-10T14:51:23Z'
          }
		]
	  }
	]
  }
}
```
Python represents JSON as as nested dictionaries and lists. I used subsetting to hone in on the desired values. Values were stored in multiple TSV files. 

Adjusting the API query for each step, I retreived the following data sets:
  
#### Central Nodes  

The hand-selected articles. Includes `r nrow(cn)` rows.

```{r echo = FALSE}
cn <- cn %>%
  select(Central.Node, Length.CN)
knitr::kable(cn[1:5, ], align = 'cc')
```
  
#### Semantic Content Nodes  
  
The non-English equivalents of the CN. Includes `r nrow(scn)` rows.

```{r echo = FALSE}
scn <- scn %>%
  select(Central.Node, Language.SCN, Semantic.Content.Node, Length.SCN)
knitr::kable(scn[15:20, ], align = 'cccc')
```

#### Local Network

All English Wikipedia articles that can be reached through links in the CN. Includes `r nrow(ln)` rows.

```{r echo = FALSE}
ln <- ln %>%
  select(Central.Node, Local.Network, Length.LN)
knitr::kable(ln[50:55, ], align = 'ccc')
```

#### Global Network  

All articles that can be reached through links in the SCN. Includes `r nrow(gn)` rows. 
  
```{r echo = FALSE}
gn <- gn %>%
  select(Central.Node, Language.SCN, Semantic.Content.Node,  Global.Network, Length.GN)
knitr::kable(gn[11005:11010, ], align = 'ccccc')
``` 


### Data Cleaning

I took these steps to prepare and summarize the data for use in the authors' analysis indexes.  

### Network node totals

I totalled the number of LN, SCN, and GN articles associated with each CN. 

Local network nodes, counted by CN.

```{r message = FALSE, warning = FALSE}
n.LN <- ln %>%
  group_by(Central.Node) %>%
  count() %>%
  select(Central.Node, n.LN = n)
```

Semantic content nodes, counted by CN.

```{r message = FALSE, warning = FALSE}
n.SCN <- scn %>%
  group_by(Central.Node) %>%
  count() %>%
  select(Central.Node, n.SCN = n)
```

Global network nodes, counted by CN.

```{r message = FALSE, warning = FALSE}
n.GN <- gn %>%
  group_by(Central.Node) %>%
  count() %>%
  select(Central.Node, n.GN = n)
```

I joined these into a single dataframe. 

```{r message = FALSE, warning = FALSE}
n.totals <- cn %>%
  select(Central.Node) %>%
  left_join(n.LN, by = 'Central.Node') %>%
  left_join(n.SCN, by = 'Central.Node') %>%
  left_join(n.GN, by = 'Central.Node')
```

The results show a number of `NA` values where no non-English versions existed (and therefore no GNs).

```{r message = FALSE, warning = FALSE}
knitr::kable(n.totals, align = 'lccc')
```

### Article length totals

I followed the same procedue to total the article lengths by context level and CN. 

Local network article length, by CN.

```{r message = FALSE, warning = FALSE}
v.LN <- ln %>%
  group_by(Central.Node) %>%
  summarize(v.LN = sum(Length.LN))
```

Semantic content article length, by CN.

```{r message = FALSE, warning = FALSE}
v.SCN <- scn %>%
  group_by(Central.Node) %>%
  summarize(v.SCN = sum(Length.SCN))
```

Global network article length, by CN.

```{r message = FALSE, warning = FALSE}
v.GN <- gn %>%
  group_by(Central.Node) %>%
  summarize(v.GN = sum(Length.GN))
```

I joined these as a single dataframe. 

```{r message = FALSE, warning = FALSE}
v.totals <- cn %>%
  select(Central.Node, v.CN = Length.CN) %>%
  left_join(v.LN, by = 'Central.Node') %>%
  left_join(v.SCN, by = 'Central.Node') %>%
  left_join(v.GN, by = 'Central.Node')
```

And added them to the node count totals. 

```{r message = FALSE, warning = FALSE}
cn.totals <- n.totals %>%
  left_join(v.totals, by = 'Central.Node') %>%
  select(Central.Node, v.CN, n.LN, v.LN, n.SCN, v.SCN, n.GN, v.GN)
```

```{r message = FALSE, warning = FALSE}
knitr::kable(cn.totals, align = 'lrrrrrrr')
```

```{r message = FALSE, warning = FALSE}
summary(cn.totals)
```

### Page-view totals

Here I compute a daily total of page views for the nodes in group SCN, LN, and GN that connect to each CN.

*Note: These are not included in the final analysis*

```{r message = FALSE, eval=FALSE, warning = FALSE}
views.cn$Datetime <- parse_date_time(views.cn$Timestamp, orders = 'YmdH')
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
a.CN <- views.cn %>%
  mutate(Date = date(Datetime)) %>%
  select(Central.Node, Date, Views) %>%
  group_by(Date, Central.Node) %>%
  summarize(a.CN = sum(Views))
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
views.scn$Datetime <- parse_date_time(views.scn$Timestamp, "YmdH")
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
a.SCN <- views.scn %>%
  mutate(Date = date(Datetime)) %>%
  select(Central.Node, Date, Views) %>%
  group_by(Date, Central.Node) %>%
  summarize(a.SCN = sum(Views))
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
views.ln$Datetime <- parse_date_time(views.ln$Timestamp, "YmdH")
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
a.LN <- views.ln %>%
  mutate(Date = date(Datetime)) %>%
  select(Central.Node, Date, Views) %>%
  group_by(Date, Central.Node) %>%
  summarize(a.LN = sum(Views))
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
views.gn$Datetime <- parse_date_time(views.gn$Timestamp, "YmdH")
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
a.GN <- views.gn %>%
  mutate(Date = date(Datetime)) %>%
  select(Central.Node, Date, Views) %>%
  group_by(Date, Central.Node) %>%
  summarize(a.GN = sum(Views))
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
a.totals <- a.CN %>%
  left_join(a.LN, by = 'Central.Node') %>%
  left_join(a.SCN, by = 'Central.Node') %>%
  left_join(a.GN, by = 'Central.Node')
```

### Edit-event totals

I resampled the edit history to a daily resolution  

*Also not included in analysis*
  
```{r message = FALSE, eval=FALSE, warning = FALSE}
edits.cn <- edits.cn %>%
  mutate(Timestamp = date(Timestamp)) %>%
  group_by(Central.Node, Timestamp) %>%
  count() %>%
  select(Central.Node, Date = Timestamp, e.CN = n)
```
```{r message = FALSE, eval=FALSE, warning = FALSE}
edits.scn <- edits.scn %>%
  mutate(Timestamp = date(Timestamp)) %>%
  group_by(Central.Node, Timestamp) %>%
  count() %>%
  select(Central.Node, Date = Timestamp, e.CN = n)
```
```{r message = FALSE, eval=FALSE, warning = FALSE}
edits.ln <- edits.ln %>%
  mutate(Timestamp = date(Timestamp)) %>%
  group_by(Central.Node, Timestamp) %>%
  count() %>%
  select(Central.Node, Date = Timestamp, e.CN = n)
```
```{r message = FALSE, eval=FALSE, warning = FALSE}
edits.gn <- edits.gn %>%
  mutate(Timestamp = date(Timestamp)) %>%
  group_by(Central.Node, Timestamp) %>%
  count() %>%
  select(Central.Node, Date = Timestamp, e.CN = n)
```

```{r message = FALSE, eval=FALSE, warning = FALSE}
e.totals <- edits %>%
  select(Central.Node, v.CN = Length.CN) %>%
  left_join(v.LN, by = 'Central.Node') %>%
  left_join(v.SCN, by = 'Central.Node') %>%
  left_join(v.GN, by = 'Central.Node')
```

 


## Data Analysis



### Representation index

The authors' representation index represents the ratio of a CN's local representation compared to global representation. 

Measured in node totals, this shows the degree of connections for the CN over the average degree of all SCN equivalents globally.

$$ \mathrm{REP}_k = \frac{n_{\mathrm{LN}} + n_{\mathrm{SCN}}} {r_{\mathrm{GN}} + n_{\mathrm{SCN}}} = \frac{k_{\mathrm{cn}}}{\langle k_{\mathrm{SCN}} \rangle} \;\; \mathrm{with} \;\; r_{\mathrm{GN}} = \frac{n_{\mathrm{GN}}}{n_{\mathrm{SCN}}}$$

Using the summaries table I created earlier:

```{r message = FALSE, warning = FALSE}
cn.totals <- cn.totals %>%
  mutate(REP.k = (n.LN + n.SCN) / ((n.GN/n.SCN) + n.SCN))
```


REP.v is a similar measurement that uses article length. 

$$ \mathrm{REP}_{v}=v_{\mathrm{CN}}\frac{n_{\mathrm{SCN}}}{v_{\mathrm{SCN}i}} $$
From the summaries table:

```{r message = FALSE, warning = FALSE}
cn.totals <- cn.totals %>%
  left_join(groups, by = c('Central.Node'))
```

```{r message = FALSE, warning = FALSE}
cn.totals <- cn.totals %>%
  mutate(REP.v = v.CN * (n.SCN / v.SCN))
```

```{r message = FALSE, warning = FALSE}
ggplot(cn.totals) +
  geom_point(aes(x = Central.Node, y = REP.k)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_log10()
```

```{r message = FALSE, warning = FALSE}
ggplot(cn.totals) +
  geom_point(aes(x = Central.Node, y = REP.v)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_log10()
```


Both plots show low variance among the data points. Technolibertarianism is an outlier by both measures. 

## Conclusion

This is not a complete project. I learned a lot about data collection and manipulation. I had never used an API. I also had to do a lot of Googling about text encoding and TeX typeseting...but it up took too much of the project, and not enough time for analysis.  
